{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating Hashed Passwords for Authentication:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Everytime when a new user is added to the app, we need to get the user_id (usually email) and their password from them.\n",
    "* Then we need to create a `Hash` of the password and add it to the `authenticator.yml` file in order for the user to be able to log-in to the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$2b$12$tdKt5p7nfhQgcDG30FldqeTZLUio5P.vAqKXSqiUHxURaei7Npkq2']\n"
     ]
    }
   ],
   "source": [
    "from streamlit_authenticator.utilities.hasher import Hasher\n",
    "\n",
    "hashed_passwords = Hasher(['abc123']).generate()\n",
    "print(hashed_passwords) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$2b$12$N9w9IGLBfLv1tne8lzalUOm2hH4ytnvbTgQ.vs0jV6EbKt9x2QPr6']\n"
     ]
    }
   ],
   "source": [
    "hashed_passwords = Hasher(['blackpearl']).generate()\n",
    "print(hashed_passwords) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Testing the connection with Snowflake Data warehouse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import snowflake.connector\n",
    "from dotenv import load_dotenv\n",
    "import os \n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = snowflake.connector.connect(\n",
    "    user='hariharan',\n",
    "    password= os.getenv('SNOWSQL_PWD'),\n",
    "    account=os.getenv('SNOWSQL_ACCOUNT'),\n",
    "    warehouse = os.getenv('SNOWSQL_WAREHOUSE'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOOD_DELIVERY\n",
      "SNOWFLAKE\n",
      "SNOWFLAKE_SAMPLE_DATA\n"
     ]
    }
   ],
   "source": [
    "# Show all the databses present in the database\n",
    "cur = con.cursor()\n",
    "cur.execute(\"SHOW DATABASES\")\n",
    "result = cur.fetchall()\n",
    "for row in result:\n",
    "    schema_name = row[1]\n",
    "    print(schema_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORE\n",
      "INFORMATION_SCHEMA\n",
      "PUBLIC\n"
     ]
    }
   ],
   "source": [
    "# Show all the schemas present in the database\n",
    "cur = con.cursor()\n",
    "cur.execute(\"SHOW SCHEMAS IN FOOD_DELIVERY\")\n",
    "result = cur.fetchall()\n",
    "for row in result:\n",
    "    schema_name = row[1]\n",
    "    print(schema_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MENU_ITEMS\n",
      "ORDERS\n",
      "ORDER_DETAILS\n",
      "PAYMENTS\n",
      "RESTAURANTS\n",
      "REVIEWS\n",
      "USERS\n"
     ]
    }
   ],
   "source": [
    "# Show all the tables present in the schema CORE\n",
    "cur.execute(\"SHOW TABLES IN SCHEMA FOOD_DELIVERY.CORE\")\n",
    "result = cur.fetchall()\n",
    "for row in result:\n",
    "    table_name = row[1]\n",
    "    print(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73748, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ORDER_ID</th>\n",
       "      <th>USER_ID</th>\n",
       "      <th>ORDER_TIME</th>\n",
       "      <th>DELIVERY_ADDRESS</th>\n",
       "      <th>ORDER_STATUS</th>\n",
       "      <th>RESTAURANT_ID</th>\n",
       "      <th>TOTAL_AMOUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>716d868b-2b5a-4bee-bc2c-ee262c81588f</td>\n",
       "      <td>d36e1a86-6e33-434a-b9c7-3f5c30753402</td>\n",
       "      <td>2024-06-01 11:53:19</td>\n",
       "      <td>9198 Gabriela Green\\nEast Marcton, DC 16873</td>\n",
       "      <td>Delivered</td>\n",
       "      <td>R18329211</td>\n",
       "      <td>152.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0f41a3a7-954a-4e24-ad84-dba22e8dd154</td>\n",
       "      <td>1ae81af6-6b78-4695-8996-442e9e40ad3c</td>\n",
       "      <td>2024-06-01 08:11:05</td>\n",
       "      <td>366 Byrd Hills\\nNew Robert, WI 24455</td>\n",
       "      <td>Delivered</td>\n",
       "      <td>R22670500</td>\n",
       "      <td>191.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fde175ad-d2ea-4901-a5fd-5c28f44e7382</td>\n",
       "      <td>2c1492fd-c993-4d00-9086-dc105c821bae</td>\n",
       "      <td>2024-06-01 21:46:46</td>\n",
       "      <td>71211 Gregory Track\\nGreenestad, OH 72514</td>\n",
       "      <td>Delivered</td>\n",
       "      <td>R66375744</td>\n",
       "      <td>35.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99e370cd-f55e-4793-b0e6-0c41d8fd9287</td>\n",
       "      <td>523d1f1a-9edd-4811-9472-f65a52d51f15</td>\n",
       "      <td>2024-06-01 09:04:39</td>\n",
       "      <td>070 Steven Heights\\nRoachville, PW 12962</td>\n",
       "      <td>Delivered</td>\n",
       "      <td>R42644875</td>\n",
       "      <td>149.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a88b8bf1-9fdf-4c8e-87ab-ac3a390d05e0</td>\n",
       "      <td>a6c31749-eb2a-40f3-aa65-49ead5cfdc3a</td>\n",
       "      <td>2024-06-01 21:05:23</td>\n",
       "      <td>9382 Alyssa Branch\\nShellyfurt, VI 11466</td>\n",
       "      <td>Delivered</td>\n",
       "      <td>R23024716</td>\n",
       "      <td>129.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               ORDER_ID                               USER_ID  \\\n",
       "0  716d868b-2b5a-4bee-bc2c-ee262c81588f  d36e1a86-6e33-434a-b9c7-3f5c30753402   \n",
       "1  0f41a3a7-954a-4e24-ad84-dba22e8dd154  1ae81af6-6b78-4695-8996-442e9e40ad3c   \n",
       "2  fde175ad-d2ea-4901-a5fd-5c28f44e7382  2c1492fd-c993-4d00-9086-dc105c821bae   \n",
       "3  99e370cd-f55e-4793-b0e6-0c41d8fd9287  523d1f1a-9edd-4811-9472-f65a52d51f15   \n",
       "4  a88b8bf1-9fdf-4c8e-87ab-ac3a390d05e0  a6c31749-eb2a-40f3-aa65-49ead5cfdc3a   \n",
       "\n",
       "           ORDER_TIME                             DELIVERY_ADDRESS  \\\n",
       "0 2024-06-01 11:53:19  9198 Gabriela Green\\nEast Marcton, DC 16873   \n",
       "1 2024-06-01 08:11:05         366 Byrd Hills\\nNew Robert, WI 24455   \n",
       "2 2024-06-01 21:46:46    71211 Gregory Track\\nGreenestad, OH 72514   \n",
       "3 2024-06-01 09:04:39     070 Steven Heights\\nRoachville, PW 12962   \n",
       "4 2024-06-01 21:05:23     9382 Alyssa Branch\\nShellyfurt, VI 11466   \n",
       "\n",
       "  ORDER_STATUS RESTAURANT_ID  TOTAL_AMOUNT  \n",
       "0    Delivered     R18329211        152.85  \n",
       "1    Delivered     R22670500        191.50  \n",
       "2    Delivered     R66375744         35.88  \n",
       "3    Delivered     R42644875        149.57  \n",
       "4    Delivered     R23024716        129.32  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating cursor object\n",
    "cur = con.cursor()\n",
    "\n",
    "# Execute a statement that will generate a result set.\n",
    "sql = \"SELECT * FROM FOOD_DELIVERY.CORE.ORDERS\"\n",
    "cur.execute(sql)\n",
    "\n",
    "# Fetch the result set from the cursor and deliver it as the pandas DataFrame.\n",
    "df = cur.fetch_pandas_all()\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing the connection with Databricks Data warehouse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os \n",
    "import pandas as pd \n",
    "from databricks import sql \n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a connection to databricks\n",
    "connection = sql.connect(server_hostname=os.getenv(\"DATABRICKS_SERVER_HOST_NAME\"),\n",
    "                         http_path=os.getenv(\"DATABRICKS_HTTP_PATH\"),\n",
    "                         access_token=os.getenv(\"DATABRICKS_ACCESS_TOKEN\")\n",
    "                         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3g/7x619qt169344pmm32qd8wgc0000gq/T/ipykernel_8801/705794241.py:1: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(\"SELECT * FROM hive_metastore.online_food_business.menu_items\", connection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>menu_item_id</th>\n",
       "      <th>restaurant_id</th>\n",
       "      <th>item_name</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R32379007_1</td>\n",
       "      <td>R32379007</td>\n",
       "      <td>Tacos</td>\n",
       "      <td>10.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R32379007_2</td>\n",
       "      <td>R32379007</td>\n",
       "      <td>Burritos</td>\n",
       "      <td>13.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R32379007_3</td>\n",
       "      <td>R32379007</td>\n",
       "      <td>Enchiladas</td>\n",
       "      <td>14.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R32379007_4</td>\n",
       "      <td>R32379007</td>\n",
       "      <td>Quesadillas</td>\n",
       "      <td>10.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R32379007_5</td>\n",
       "      <td>R32379007</td>\n",
       "      <td>Nachos</td>\n",
       "      <td>10.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  menu_item_id restaurant_id    item_name  price\n",
       "0  R32379007_1     R32379007        Tacos  10.20\n",
       "1  R32379007_2     R32379007     Burritos  13.93\n",
       "2  R32379007_3     R32379007   Enchiladas  14.15\n",
       "3  R32379007_4     R32379007  Quesadillas  10.47\n",
       "4  R32379007_5     R32379007       Nachos  10.80"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_sql(\"SELECT * FROM hive_metastore.online_food_business.menu_items\", connection)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Retrieving the Table Schema, Categorical Column details and Sample rows for Prompt Context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os \n",
    "import pandas as pd \n",
    "from databricks import sql \n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected tables list \n",
    "catalog = \"hive_metastore\"\n",
    "schema = \"online_food_business\"\n",
    "tables_list = [\"menu_items\",\"orders\",\"users\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_schema = \"\"\n",
    "\n",
    "# # Iterating through each selected tables and get the list of columns for each table.\n",
    "# for table in tables_list:\n",
    "\n",
    "#     conn = sql.connect(server_hostname = os.getenv(\"DATABRICKS_SERVER_HOSTNAME\"),\n",
    "#                     http_path       = os.getenv(\"DATABRICKS_HTTP_PATH\"),\n",
    "#                     access_token    = os.getenv(\"DATABRICKS_ACCESS_TOKEN\"))        \n",
    "\n",
    "#     # Getting the Schema for the table\n",
    "#     query = f\"SHOW CREATE TABLE `{catalog}`.{schema}.{table}\"\n",
    "#     df = pd.read_sql(sql=query,con=conn)\n",
    "#     stmt = df['createtab_stmt'][0]\n",
    "#     stmt = stmt.split(\"USING\")[0]\n",
    "\n",
    "#     # Filtering the String columns from the table to identify Categorical columns\n",
    "#     query = f\"DESCRIBE TABLE `{catalog}`.{schema}.{table}\"\n",
    "#     df = pd.read_sql(sql=query,con=conn)\n",
    "#     string_cols = df[df['data_type']=='string']['col_name'].values.tolist()\n",
    "\n",
    "#     sql_distinct = \"\"\n",
    "#     for col in string_cols:\n",
    "#         # Getting the distinct values for each column as rows\n",
    "#         if col == string_cols[-1]:\n",
    "#             sql_distinct += f\"SELECT '{col}' AS column_name, COUNT(DISTINCT {col}) AS cnt, ARRAY_AGG(DISTINCT {col}) AS values FROM `{catalog}`.{schema}.{table}\"\n",
    "#         else:\n",
    "#             sql_distinct += f\"SELECT '{col}' AS column_name, COUNT(DISTINCT {col}) AS cnt, ARRAY_AGG(DISTINCT {col}) AS values FROM `{catalog}`.{schema}.{table} UNION ALL \"\n",
    "\n",
    "#     # print(sql_distinct)\n",
    "#     df_categories = pd.read_sql(sql=sql_distinct,con=conn)\n",
    "#     df_categories = df_categories[df_categories['cnt'] <= 20]\n",
    "#     df_categories = df_categories.drop(columns='cnt')\n",
    "\n",
    "#     if df_categories.empty:\n",
    "#         df_categories_string = \"No Categorical Fields\"\n",
    "#     else:\n",
    "#         df_categories_string = df_categories.to_string(index=False)\n",
    "\n",
    "\n",
    "#     # Getting the sample rows from the table\n",
    "#     query = f\"SELECT * FROM `{catalog}`.{schema}.{table} LIMIT 3\"\n",
    "#     df = pd.read_sql(sql=query,con=conn)\n",
    "#     sample_rows = df.to_string(index=False)\n",
    "\n",
    "    \n",
    "#     # df_categories = df_string.groupby('col_name').filter(lambda x: x['col_name'].count() <= 20)\n",
    "#     # print(df_string)\n",
    "#     # print(df_categories)\n",
    "#     if table_schema == \"\":\n",
    "#         table_schema = stmt + \"\\n\" + sample_rows + \"\\n\\nCategorical Fields:\\n\" + df_categories_string + \"\\n\"\n",
    "#     else:\n",
    "#         table_schema = table_schema + \"\\n\" + stmt + \"\\n\" + sample_rows + \"\\n\\nCategorical Fields:\\n\" + df_categories_string + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create enriched database schema details for the Prompt\n",
    "def get_enriched_database_schema(catalog,schema,tables_list):\n",
    "    table_schema = \"\"\n",
    "\n",
    "    # Iterating through each selected tables and get the list of columns for each table.\n",
    "    for table in tables_list:\n",
    "\n",
    "        conn = sql.connect(server_hostname = os.getenv(\"DATABRICKS_SERVER_HOSTNAME\"),\n",
    "                        http_path       = os.getenv(\"DATABRICKS_HTTP_PATH\"),\n",
    "                        access_token    = os.getenv(\"DATABRICKS_ACCESS_TOKEN\"))        \n",
    "\n",
    "        # Getting the Schema for the table\n",
    "        query = f\"SHOW CREATE TABLE `{catalog}`.{schema}.{table}\"\n",
    "        df = pd.read_sql(sql=query,con=conn)\n",
    "        stmt = df['createtab_stmt'][0]\n",
    "        stmt = stmt.split(\"USING\")[0]\n",
    "\n",
    "        # Filtering the String columns from the table to identify Categorical columns\n",
    "        query = f\"DESCRIBE TABLE `{catalog}`.{schema}.{table}\"\n",
    "        df = pd.read_sql(sql=query,con=conn)\n",
    "        string_cols = df[df['data_type']=='string']['col_name'].values.tolist()\n",
    "\n",
    "        sql_distinct = \"\"\n",
    "        for col in string_cols:\n",
    "            # Getting the distinct values for each column as rows\n",
    "            if col == string_cols[-1]:\n",
    "                sql_distinct += f\"SELECT '{col}' AS column_name, COUNT(DISTINCT {col}) AS cnt, ARRAY_AGG(DISTINCT {col}) AS values FROM `{catalog}`.{schema}.{table}\"\n",
    "            else:\n",
    "                sql_distinct += f\"SELECT '{col}' AS column_name, COUNT(DISTINCT {col}) AS cnt, ARRAY_AGG(DISTINCT {col}) AS values FROM `{catalog}`.{schema}.{table} UNION ALL \"\n",
    "\n",
    "        # print(sql_distinct)\n",
    "        df_categories = pd.read_sql(sql=sql_distinct,con=conn)\n",
    "        df_categories = df_categories[df_categories['cnt'] <= 20]\n",
    "        df_categories = df_categories.drop(columns='cnt')\n",
    "\n",
    "        if df_categories.empty:\n",
    "            df_categories_string = \"No Categorical Fields\"\n",
    "        else:\n",
    "            df_categories_string = df_categories.to_string(index=False)\n",
    "\n",
    "\n",
    "        # Getting the sample rows from the table\n",
    "        query = f\"SELECT * FROM `{catalog}`.{schema}.{table} LIMIT 3\"\n",
    "        df = pd.read_sql(sql=query,con=conn)\n",
    "        sample_rows = df.to_string(index=False)\n",
    "\n",
    "        if table_schema == \"\":\n",
    "            table_schema = stmt + \"\\n\" + sample_rows + \"\\n\\nCategorical Fields:\\n\" + df_categories_string + \"\\n\"\n",
    "        else:\n",
    "            table_schema = table_schema + \"\\n\" + stmt + \"\\n\" + sample_rows + \"\\n\\nCategorical Fields:\\n\" + df_categories_string + \"\\n\"\n",
    "    \n",
    "    return table_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3g/7x619qt169344pmm32qd8wgc0000gq/T/ipykernel_19961/3861049042.py:14: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(sql=query,con=conn)\n",
      "/var/folders/3g/7x619qt169344pmm32qd8wgc0000gq/T/ipykernel_19961/3861049042.py:20: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(sql=query,con=conn)\n",
      "/var/folders/3g/7x619qt169344pmm32qd8wgc0000gq/T/ipykernel_19961/3861049042.py:32: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_categories = pd.read_sql(sql=sql_distinct,con=conn)\n",
      "/var/folders/3g/7x619qt169344pmm32qd8wgc0000gq/T/ipykernel_19961/3861049042.py:44: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(sql=query,con=conn)\n",
      "/var/folders/3g/7x619qt169344pmm32qd8wgc0000gq/T/ipykernel_19961/3861049042.py:14: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(sql=query,con=conn)\n",
      "/var/folders/3g/7x619qt169344pmm32qd8wgc0000gq/T/ipykernel_19961/3861049042.py:20: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(sql=query,con=conn)\n",
      "/var/folders/3g/7x619qt169344pmm32qd8wgc0000gq/T/ipykernel_19961/3861049042.py:32: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_categories = pd.read_sql(sql=sql_distinct,con=conn)\n",
      "/var/folders/3g/7x619qt169344pmm32qd8wgc0000gq/T/ipykernel_19961/3861049042.py:44: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(sql=query,con=conn)\n",
      "/var/folders/3g/7x619qt169344pmm32qd8wgc0000gq/T/ipykernel_19961/3861049042.py:14: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(sql=query,con=conn)\n",
      "/var/folders/3g/7x619qt169344pmm32qd8wgc0000gq/T/ipykernel_19961/3861049042.py:20: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(sql=query,con=conn)\n",
      "/var/folders/3g/7x619qt169344pmm32qd8wgc0000gq/T/ipykernel_19961/3861049042.py:32: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_categories = pd.read_sql(sql=sql_distinct,con=conn)\n",
      "/var/folders/3g/7x619qt169344pmm32qd8wgc0000gq/T/ipykernel_19961/3861049042.py:44: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(sql=query,con=conn)\n"
     ]
    }
   ],
   "source": [
    "table_schema = get_enriched_database_schema(catalog,schema,tables_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE hive_metastore.online_food_business.menu_items (\n",
      "  menu_item_id STRING,\n",
      "  restaurant_id STRING,\n",
      "  item_name STRING,\n",
      "  price DOUBLE)\n",
      "\n",
      "menu_item_id restaurant_id  item_name  price\n",
      " R32379007_1     R32379007      Tacos  10.20\n",
      " R32379007_2     R32379007   Burritos  13.93\n",
      " R32379007_3     R32379007 Enchiladas  14.15\n",
      "\n",
      "Categorical Fields:\n",
      "No Categorical Fields\n",
      "\n",
      "CREATE TABLE hive_metastore.online_food_business.orders (\n",
      "  order_id STRING,\n",
      "  user_id STRING,\n",
      "  order_time TIMESTAMP,\n",
      "  delivery_address STRING,\n",
      "  order_status STRING,\n",
      "  restaurant_id STRING,\n",
      "  total_amount DOUBLE)\n",
      "\n",
      "                            order_id                              user_id                order_time                            delivery_address order_status restaurant_id  total_amount\n",
      "716d868b-2b5a-4bee-bc2c-ee262c81588f d36e1a86-6e33-434a-b9c7-3f5c30753402 2024-06-01 11:53:19+00:00 9198 Gabriela Green\\nEast Marcton, DC 16873    Delivered     R18329211        152.85\n",
      "0f41a3a7-954a-4e24-ad84-dba22e8dd154 1ae81af6-6b78-4695-8996-442e9e40ad3c 2024-06-01 08:11:05+00:00        366 Byrd Hills\\nNew Robert, WI 24455    Delivered     R22670500        191.50\n",
      "fde175ad-d2ea-4901-a5fd-5c28f44e7382 2c1492fd-c993-4d00-9086-dc105c821bae 2024-06-01 21:46:46+00:00   71211 Gregory Track\\nGreenestad, OH 72514    Delivered     R66375744         35.88\n",
      "\n",
      "Categorical Fields:\n",
      " column_name                                       values\n",
      "order_status [Cancelled, Pending, Undelivered, Delivered]\n",
      "\n",
      "CREATE TABLE hive_metastore.online_food_business.users (\n",
      "  user_id STRING,\n",
      "  name STRING,\n",
      "  gender STRING,\n",
      "  email STRING,\n",
      "  phone_number STRING,\n",
      "  delivery_address STRING)\n",
      "\n",
      "                             user_id              name gender                         email        phone_number                                       delivery_address\n",
      "2c9e3b84-16d3-4f4c-858e-82dffa36f991     Daniel Molina   Male       daniel.molina@gmail.com  752.300.5266x02508 738 Carly Island Apt. 434\\nSouth Melissaberg, GU 31880\n",
      "752cffa9-d31c-45e1-b0ab-3e35567b90f4 Michael Rodriguez   Male michael.rodriguez@hotmail.com +1-776-822-7329x203      6504 Rachel Burg Apt. 042\\nCynthiaburgh, PA 66026\n",
      "bc304a8c-ad89-4757-b8c2-481c38435419     Charles Patel   Male     charles.patel@hotmail.com  774-219-6812x63426             366 Ryan Field\\nNorth Danielside, OK 02127\n",
      "\n",
      "Categorical Fields:\n",
      "column_name         values\n",
      "     gender [Female, Male]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(table_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def create_sql(question,table_schema):\n",
    "\n",
    "    ### Defining the prompt template\n",
    "    template_string = \"\"\" \n",
    "    You are a expert data engineer working with a Databricks environment.\\\n",
    "    Your task is to generate a working SQL query in Databricks SQL dialect. \\\n",
    "    During join if column name are same please use alias ex llm.customer_id \\\n",
    "    in select statement. It is also important to respect the type of columns: \\\n",
    "    if a column is string, the value should be enclosed in quotes. \\\n",
    "    If you are writing CTEs then include all the required columns. \\\n",
    "    While concatenating a non string column, make sure cast the column to string. \\\n",
    "    For date columns comparing to string , please cast the string input.\\\n",
    "    For string columns, check if it is a categorical column and only use the appropriate values provided in the schema.\\\n",
    "\n",
    "    SCHEMA:\n",
    "    ## {table_schema} ##\n",
    "\n",
    "    QUESTION:\n",
    "    ##\n",
    "    {question}\n",
    "    ##\n",
    "\n",
    "\n",
    "    IMPORTANT: MAKE SURE THE OUTPUT IS JUST THE SQL CODE AND NOTHING ELSE. Ensure the appropriate CATALOG is used in the query and SCHEMA is specified when reading the tables.\n",
    "    ##\n",
    "\n",
    "    OUTPUT:\n",
    "    \"\"\"\n",
    "    prompt_template = PromptTemplate.from_template(template_string)\n",
    "\n",
    "    ### Defining the LLM chain\n",
    "    llm_chain = LLMChain(\n",
    "        llm=ChatOpenAI(model=\"gpt-4o-mini\",temperature=0),\n",
    "        prompt=prompt_template\n",
    "    )\n",
    "\n",
    "    response =  llm_chain.invoke({\"question\":question,\"table_schema\":table_schema})\n",
    "    output = response['text']\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = create_sql(\"What is the total orders by different restaurants by every day (in separate columns)\",table_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```sql\n",
      "WITH daily_orders AS (\n",
      "    SELECT \n",
      "        DATE(order_time) AS order_date,\n",
      "        restaurant_id,\n",
      "        COUNT(order_id) AS total_orders\n",
      "    FROM \n",
      "        hive_metastore.online_food_business.orders\n",
      "    GROUP BY \n",
      "        DATE(order_time), restaurant_id\n",
      ")\n",
      "\n",
      "SELECT \n",
      "    do.order_date,\n",
      "    do.restaurant_id,\n",
      "    do.total_orders\n",
      "FROM \n",
      "    daily_orders AS do\n",
      "ORDER BY \n",
      "    do.order_date, do.restaurant_id;\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to render the sql code\n",
    "def process_llm_response_for_sql(response: str) -> str:\n",
    "    # Extract the Mermaid code block from the response\n",
    "    start_idx = response.find(\"```sql\") + len(\"```sql\")\n",
    "    end_idx = response.find(\"```\", start_idx)\n",
    "    sql_code = response[start_idx:end_idx].strip()\n",
    "\n",
    "    return sql_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITH daily_orders AS (\n",
      "    SELECT \n",
      "        DATE(order_time) AS order_date,\n",
      "        restaurant_id,\n",
      "        COUNT(order_id) AS total_orders\n",
      "    FROM \n",
      "        hive_metastore.online_food_business.orders\n",
      "    GROUP BY \n",
      "        DATE(order_time), restaurant_id\n",
      ")\n",
      "\n",
      "SELECT \n",
      "    do.order_date,\n",
      "    do.restaurant_id,\n",
      "    do.total_orders\n",
      "FROM \n",
      "    daily_orders AS do\n",
      "ORDER BY \n",
      "    do.order_date, do.restaurant_id;\n"
     ]
    }
   ],
   "source": [
    "final_query_1 = process_llm_response_for_sql(result)\n",
    "print(final_query_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_advanced_sql(question,sql_code,table_schema):\n",
    "\n",
    "    ### Defining the prompt template\n",
    "    template_string = \"\"\" \n",
    "    You are a expert data engineer working with a Databricks environment.\\\n",
    "    Your task is to generate a working SQL query in Databricks SQL dialect. \\\n",
    "    Enclose the complete SQL_CODE in a WITH clause and name it as MASTER. DON'T ALTER THE given SQL_CODE. \\\n",
    "    Then based on the QUESTION and the master WITH clause, generate the final SQL query based on the WITH clause.\\\n",
    "    ONLY IF additional information is needed to answer the QUESTION, then use the SCHEMA to join the details to get the final answer. \\\n",
    "\n",
    "\n",
    "    INPUT:\n",
    "    SQL_CODE:\n",
    "    ##\n",
    "    {sql_code}\n",
    "    ##\n",
    "\n",
    "    SCHEMA:\n",
    "    ## {table_schema} ##\n",
    "\n",
    "    QUESTION:\n",
    "    ##\n",
    "    {question}\n",
    "    ##\n",
    "\n",
    "    IMPORTANT: MAKE SURE THE OUTPUT IS JUST THE SQL CODE AND NOTHING ELSE.\n",
    "    ##\n",
    "\n",
    "\n",
    "    OUTPUT:\n",
    "    \"\"\"\n",
    "    prompt_template = PromptTemplate.from_template(template_string)\n",
    "\n",
    "    ### Defining the LLM chain\n",
    "    llm_chain = LLMChain(\n",
    "        llm=ChatOpenAI(model=\"gpt-4o-mini\",temperature=0),\n",
    "        prompt=prompt_template\n",
    "    )\n",
    "\n",
    "    response =  llm_chain.invoke({\"sql_code\":sql_code,\"question\":question,\"table_schema\":table_schema})\n",
    "    output = response['text']\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_query_2 = create_advanced_sql(question=\"Can you give me the average total orders overall and also average item quantity?\",sql_code=final_query_1,table_schema=table_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITH MASTER AS (\n",
      "    WITH daily_orders AS (\n",
      "    SELECT \n",
      "        DATE(order_time) AS order_date,\n",
      "        restaurant_id,\n",
      "        COUNT(order_id) AS total_orders\n",
      "    FROM \n",
      "        hive_metastore.online_food_business.orders\n",
      "    GROUP BY \n",
      "        DATE(order_time), restaurant_id\n",
      ")\n",
      "\n",
      "SELECT \n",
      "    do.order_date,\n",
      "    do.restaurant_id,\n",
      "    do.total_orders\n",
      "FROM \n",
      "    daily_orders AS do\n",
      "ORDER BY \n",
      "    do.order_date, do.restaurant_id\n",
      ")\n",
      "\n",
      "SELECT \n",
      "    AVG(total_orders) AS average_total_orders,\n",
      "    AVG(item_quantity) AS average_item_quantity\n",
      "FROM (\n",
      "    SELECT \n",
      "        do.restaurant_id,\n",
      "        do.total_orders,\n",
      "        COUNT(mi.menu_item_id) AS item_quantity\n",
      "    FROM \n",
      "        MASTER AS do\n",
      "    JOIN \n",
      "        hive_metastore.online_food_business.menu_items AS mi\n",
      "    ON \n",
      "        do.restaurant_id = mi.restaurant_id\n",
      "    GROUP BY \n",
      "        do.restaurant_id, do.total_orders\n",
      ") AS subquery;\n"
     ]
    }
   ],
   "source": [
    "final_query_2 = process_llm_response_for_sql(final_query_2)\n",
    "print(final_query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITH MASTER AS (\n",
      "    WITH daily_orders AS (\n",
      "    SELECT \n",
      "        DATE(order_time) AS order_date,\n",
      "        restaurant_id,\n",
      "        COUNT(order_id) AS total_orders\n",
      "    FROM \n",
      "        hive_metastore.online_food_business.orders\n",
      "    GROUP BY \n",
      "        DATE(order_time), restaurant_id\n",
      ")\n",
      "\n",
      "SELECT \n",
      "    do.order_date,\n",
      "    do.restaurant_id,\n",
      "    do.total_orders\n",
      "FROM \n",
      "    daily_orders AS do\n",
      "ORDER BY \n",
      "    do.order_date, do.restaurant_id\n",
      ")\n",
      "\n",
      "SELECT \n",
      "    AVG(total_orders) AS average_total_orders,\n",
      "    AVG(item_quantity) AS average_item_quantity\n",
      "FROM (\n",
      "    SELECT \n",
      "        do.restaurant_id,\n",
      "        do.total_orders,\n",
      "        COUNT(mi.menu_item_id) AS item_quantity\n",
      "    FROM \n",
      "        MASTER AS do\n",
      "    JOIN \n",
      "        hive_metastore.online_food_business.menu_items AS mi\n",
      "    ON \n",
      "        do.restaurant_id = mi.restaurant_id\n",
      "    GROUP BY \n",
      "        do.restaurant_id, do.total_orders\n",
      ") AS subquery;\n"
     ]
    }
   ],
   "source": [
    "print(final_query_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Query Self-Correction Component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_query(query):\n",
    "    # Getting the sample details of the selected table\n",
    "    conn = sql.connect(server_hostname = os.getenv(\"DATABRICKS_SERVER_HOSTNAME\"),\n",
    "                    http_path       = os.getenv(\"DATABRICKS_HTTP_PATH\"),\n",
    "                    access_token    = os.getenv(\"DATABRICKS_ACCESS_TOKEN\"))\n",
    "\n",
    "    # query = query.replace(\";\",\"\")\n",
    "    # query = query + f\" LIMIT 1000;\"\n",
    "    df = pd.read_sql(sql=query,con=conn)\n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_correction(query):\n",
    "    error_msg = \"\"\n",
    "\n",
    "    try:\n",
    "        df = load_data_from_query(query)\n",
    "        print(df.shape)\n",
    "        # df.head()\n",
    "        error_msg += \"Successful\"\n",
    "    except Exception as e:\n",
    "        error_msg += str(e)\n",
    "    \n",
    "    if error_msg == \"Successful\":\n",
    "        return error_msg\n",
    "    else:\n",
    "        # print(\"There is error\")\n",
    "        # print(error_msg)\n",
    "        return error_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = final_query_1\n",
    "query = \"SELECT * FROM hive_metastore.online_food_business.menu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3g/7x619qt169344pmm32qd8wgc0000gq/T/ipykernel_19961/1382692826.py:9: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(sql=query,con=conn)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Execution failed on sql: SELECT * FROM hive_metastore.online_food_business.menu\\n[TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`online_food_business`.`menu` cannot be found. Verify the spelling and correctness of the schema and catalog.\\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 1 pos 14\\nunable to rollback'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_msg = self_correction(query)\n",
    "error_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_sql(question,sql_code,table_schema,error_msg):\n",
    "\n",
    "    ### Defining the prompt template\n",
    "    template_string = \"\"\" \n",
    "    You are a expert data engineer working with a Databricks environment.\\\n",
    "    Your task is to modify the SQL_CODE using Databricks SQL dialect based on the QUESTION, SCHEMA and the ERROR_MESSAGE. \\\n",
    "    If ERROR_MESSAGE is provided, then make sure to correct the SQL query according to that. \\\n",
    "\n",
    "    SCHEMA:\n",
    "    ## {table_schema} ##\n",
    "\n",
    "    ERROR_MESSAGE:\n",
    "    ## {error_msg} ##\n",
    "\n",
    "    SQL_CODE:\n",
    "    ##\n",
    "    {sql_code}\n",
    "\n",
    "    QUESTION:\n",
    "    ## {question} ##\n",
    "\n",
    "    ##\n",
    "\n",
    "\n",
    "    IMPORTANT: MAKE SURE THE OUTPUT IS JUST THE SQL CODE AND NOTHING ELSE. Ensure the appropriate CATALOG is used in the query and SCHEMA is specified when reading the tables.\n",
    "    ##\n",
    "\n",
    "    OUTPUT:\n",
    "    \"\"\"\n",
    "    prompt_template = PromptTemplate.from_template(template_string)\n",
    "\n",
    "    ### Defining the LLM chain\n",
    "    llm_chain = LLMChain(\n",
    "        llm=ChatOpenAI(model=\"gpt-4o-mini\",temperature=0),\n",
    "        prompt=prompt_template\n",
    "    )\n",
    "\n",
    "    response =  llm_chain.invoke({\"question\":question,\"sql_code\":sql_code,\"table_schema\":table_schema,\"error_msg\":error_msg})\n",
    "    output = response['text']\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT * FROM hive_metastore.online_food_business.menu'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified = correct_sql(\"List the details of the menu item table\",query,table_schema,error_msg=error_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```sql\n",
      "SELECT * FROM hive_metastore.online_food_business.menu_items\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3g/7x619qt169344pmm32qd8wgc0000gq/T/ipykernel_19961/1382692826.py:9: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(sql=query,con=conn)\n"
     ]
    }
   ],
   "source": [
    "# Final\n",
    "error_msg = self_correction(query)\n",
    "\n",
    "if error_msg == \"Successful\":\n",
    "    print(\"Query is successful\")\n",
    "else:\n",
    "    modified_query = correct_sql(\"List the details of the menu item table\",query,table_schema,error_msg=error_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final function to validate and self-correct\n",
    "def validate_and_correct_sql(query,table_schema):\n",
    "    error_msg = self_correction(query)\n",
    "\n",
    "    if error_msg == \"Successful\":\n",
    "        # print(\"Query is successful\")\n",
    "        return \"Correct\",query\n",
    "    else:\n",
    "        modified_query = correct_sql(\"List the details of the menu item table\",query,table_schema,error_msg=error_msg)\n",
    "        return \"Incorrect\",modified_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```sql\n",
      "SELECT * FROM hive_metastore.online_food_business.menu_items\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(modified_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Adding Favourites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_query(query):\n",
    "    # Getting the sample details of the selected table\n",
    "    conn = sql.connect(server_hostname = os.getenv(\"DATABRICKS_SERVER_HOSTNAME\"),\n",
    "                    http_path       = os.getenv(\"DATABRICKS_HTTP_PATH\"),\n",
    "                    access_token    = os.getenv(\"DATABRICKS_ACCESS_TOKEN\"))\n",
    "\n",
    "    # query = query.replace(\";\",\"\")\n",
    "    # query = query + f\" LIMIT 1000;\"\n",
    "    df = pd.read_sql(sql=query,con=conn)\n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_user_history(user_name,question,query,favourite_ind):\n",
    "    conn = sql.connect(server_hostname = os.getenv(\"DATABRICKS_SERVER_HOSTNAME\"),\n",
    "                    http_path       = os.getenv(\"DATABRICKS_HTTP_PATH\"),\n",
    "                    access_token    = os.getenv(\"DATABRICKS_ACCESS_TOKEN\"))\n",
    "    \n",
    "    user_history_table = \"hive_metastore.dev_tools.sqlgenpro_user_query_history\"\n",
    "\n",
    "    query = f\"INSERT INTO {user_history_table} VALUES ('{user_name}',current_timestamp(),'{question}','{query}',{favourite_ind})\"\n",
    "    # query = f\"SELECT * FROM {user_history_table}\"\n",
    "    df = pd.read_sql(sql=query,con=conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3g/7x619qt169344pmm32qd8wgc0000gq/T/ipykernel_19961/3664673451.py:10: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(sql=query,con=conn)\n"
     ]
    }
   ],
   "source": [
    "add_to_user_history(\"hariharan\",\"List the details of the menu item table\",query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sqlgenpro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
